{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2a957a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Projects/denk_baseline/venv/lib/python3.10/site-packages/controlnet_aux/mediapipe_face/mediapipe_face_common.py:7: UserWarning: The module 'mediapipe' is not installed. The package will have limited functionality. Please install it using the command: pip install 'mediapipe'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "from diffusers import ControlNetModel, DDIMScheduler, StableDiffusionPipeline\n",
    "\n",
    "from iattention import UNION_PIPELINES\n",
    "from iattention.utils import correct_colors_hist, show_image, show_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94df62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_info(cap):\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    return height, width, fps, frame_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9beb8514",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load('../configs/default.yaml')\n",
    "config['pipe_config']['total_steps'] = config['common']['num_inference_steps']\n",
    "config['unet_config']['total_steps'] = config['common']['num_inference_steps']\n",
    "config['controlnet_config']['total_steps'] = config['common']['num_inference_steps']\n",
    "config['common']['unet_from'] = None #'../models/deliberate_v2.safetensors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64e3711f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to Intel/dpt-large and revision e93beec (https://huggingface.co/Intel/dpt-large).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ae34e9f0f74c9994929c7c814c8084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'iattention.stablediffusion_controlnet_pipeline.IAttentionSDCPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "pipe = UNION_PIPELINES[config['pipeline']](config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "464312e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[ VIDEO INFO | WxH: 750x720 | FPS: 30 | FRAME COUNT: 36 ]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 36/36 [02:27<00:00,  4.09s/it]\n"
     ]
    }
   ],
   "source": [
    "config['common']['input_video_path'] = '../video/input/man.mp4'\n",
    "config['common']['output_video_path'] = '../video/output/video-from-jupyter.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(config['common']['input_video_path'])\n",
    "orig_height, orig_width, fps, frame_count = get_video_info(cap)\n",
    "\n",
    "print(f'\\n\\n[ VIDEO INFO | WxH: {orig_width}x{orig_height} | FPS: {fps} | FRAME COUNT: {frame_count} ]\\n\\n')\n",
    "\n",
    "img_h, img_w = config['common']['img_h'], config['common']['img_w']\n",
    "out_h, out_w = img_h, img_w\n",
    "if config['common']['original_output_size']:\n",
    "    out_h, out_w = orig_height, orig_width\n",
    "\n",
    "writer = cv2.VideoWriter(\n",
    "    config['common']['output_video_path'], \n",
    "    cv2.VideoWriter_fourcc(*'mp4v'), \n",
    "    fps, \n",
    "    (out_w * 2, out_h),\n",
    ")\n",
    "\n",
    "first_image = None\n",
    "for _ in tqdm(range(frame_count)):\n",
    "    c_ret, c_image = cap.read()\n",
    "    if not c_ret or c_image is None:\n",
    "        break\n",
    "    image = cv2.resize(c_image, (img_h, img_w))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    result = pipe(image)\n",
    "\n",
    "    if first_image is None:\n",
    "        first_image = result.copy()\n",
    "    else:\n",
    "        predict_image = correct_colors_hist(first_image, result, config['common']['hist_normalize'])\n",
    "            \n",
    "    out_images = [cv2.resize(x, (out_w, out_h)) for x in [image, result]]\n",
    "    result = np.hstack(out_images)\n",
    "    result = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    writer.write(result)\n",
    "    \n",
    "\n",
    "cap.release()\n",
    "writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc5b975",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfd303d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1d3fe2-2bf1-4c11-949d-f57e6115de8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
